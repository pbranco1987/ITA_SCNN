@article{li_pruning_2016,
	title = {Pruning {Filters} for {Efficient} {ConvNets}},
	url = {https://openreview.net/forum?id=rJqFGTslg},
	abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning...},
	language = {en},
	urldate = {2020-10-22},
	author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
	month = nov,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\pbran\\Zotero\\storage\\2RS5A6GZ\\Li et al. - 2016 - Pruning Filters for Efficient ConvNets.pdf:application/pdf;Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\J5E5U8MN\\forum.html:text/html}
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	urldate = {2020-10-22},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02406},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information Theory Workshop (ITW) (IEEE ITW 2015)},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbran\\Zotero\\storage\\AZ4GFJJG\\Tishby e Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\F7759YN8\\1503.html:text/html}
}

@misc{noauthor_partial_nodate,
	title = {Partial information decomposition as a unified approach to the specification of neural goal functions {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S027826261530021X?token=5A626F3F46FE6565424527870E356894B6B5F43C58EFCAD207BFAA617E19A3E043C0704716DF3B55FF55B4FB914A59F5},
	language = {en},
	urldate = {2020-10-22},
	doi = {10.1016/j.bandc.2015.09.004},
	file = {Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\W2MXJZ5M\\S027826261530021X.html:text/html;Texto completo:C\:\\Users\\pbran\\Zotero\\storage\\Y2NFHI8I\\Partial information decomposition as a unified app.pdf:application/pdf}
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	url = {http://arxiv.org/abs/1703.00810},
	abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the {\textbackslash}textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{{\textbackslash}emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
	urldate = {2020-10-22},
	journal = {arXiv:1703.00810 [cs]},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.00810},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 19 pages, 8 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbran\\Zotero\\storage\\WWICYLZE\\Shwartz-Ziv e Tishby - 2017 - Opening the Black Box of Deep Neural Networks via .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\M8QEVW9E\\1703.html:text/html}
}

@article{saxe_information_2019,
	title = {On the information bottleneck theory of deep learning},
	volume = {2019},
	issn = {1742-5468},
	url = {https://iopscience.iop.org/article/10.1088/1742-5468/ab3985},
	doi = {10.1088/1742-5468/ab3985},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three speciﬁc claims: ﬁrst, that deep networks undergo two distinct phases consisting of an initial ﬁtting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we ﬁnd that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB ﬁndings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the ﬁtting process rather than during a subsequent compression period.},
	language = {en},
	number = {12},
	urldate = {2020-10-22},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
	month = dec,
	year = {2019},
	pages = {124020},
	file = {Saxe et al. - 2019 - On the information bottleneck theory of deep learn.pdf:C\:\\Users\\pbran\\Zotero\\storage\\Z3I7J6Y7\\Saxe et al. - 2019 - On the information bottleneck theory of deep learn.pdf:application/pdf}
}

@article{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	urldate = {2020-10-22},
	journal = {arXiv:physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv: physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbran\\Zotero\\storage\\RHMLHWMT\\Tishby et al. - 2000 - The information bottleneck method.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\CX93RGAL\\0004057.html:text/html}
}

@incollection{gabrie_entropy_2018,
	title = {Entropy and mutual information in models of deep neural networks},
	url = {http://papers.nips.cc/paper/7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.pdf},
	urldate = {2020-10-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Gabrié, Marylou and Manoel, Andre and Luneau, Clément and barbier, jean and Macris, Nicolas and Krzakala, Florent and Zdeborová, Lenka},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1821--1831},
	file = {NIPS Full Text PDF:C\:\\Users\\pbran\\Zotero\\storage\\QM432MQE\\Gabrié et al. - 2018 - Entropy and mutual information in models of deep n.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\3MRCZY8R\\7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.html:text/html}
}

@misc{researcher_tiny_2020,
	title = {Tiny {Machine} {Learning}: {The} {Next} {AI} {Revolution}},
	shorttitle = {Tiny {Machine} {Learning}},
	url = {https://towardsdatascience.com/tiny-machine-learning-the-next-ai-revolution-495c26463868},
	abstract = {The bigger model is not always the better model},
	language = {en},
	urldate = {2020-10-22},
	journal = {Medium},
	author = {Researcher, PhD, Matthew Stewart},
	month = oct,
	year = {2020},
	file = {Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\CDX78MAQ\\tiny-machine-learning-the-next-ai-revolution-495c26463868.html:text/html}
}

@article{wang_scnn_2019,
	title = {{SCNN}: {A} {General} {Distribution} based {Statistical} {Convolutional} {Neural} {Network} with {Application} to {Video} {Object} {Detection}},
	shorttitle = {{SCNN}},
	url = {http://arxiv.org/abs/1903.07663},
	abstract = {Various convolutional neural networks (CNNs) were developed recently that achieved accuracy comparable with that of human beings in computer vision tasks such as image recognition, object detection and tracking, etc. Most of these networks, however, process one single frame of image at a time, and may not fully utilize the temporal and contextual correlation typically present in multiple channels of the same image or adjacent frames from a video, thus limiting the achievable throughput. This limitation stems from the fact that existing CNNs operate on deterministic numbers. In this paper, we propose a novel statistical convolutional neural network (SCNN), which extends existing CNN architectures but operates directly on correlated distributions rather than deterministic numbers. By introducing a parameterized canonical model to model correlated data and defining corresponding operations as required for CNN training and inference, we show that SCNN can process multiple frames of correlated images effectively, hence achieving significant speedup over existing CNN models. We use a CNN based video object detection as an example to illustrate the usefulness of the proposed SCNN as a general network model. Experimental results show that even a non-optimized implementation of SCNN can still achieve 178\% speedup over existing CNNs with slight accuracy degradation.},
	urldate = {2020-10-22},
	journal = {arXiv:1903.07663 [cs, stat]},
	author = {Wang, Tianchen and Xiong, Jinjun and Xu, Xiaowei and Shi, Yiyu},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07663},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: AAAI19},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbran\\Zotero\\storage\\SX6XB4QE\\Wang et al. - 2019 - SCNN A General Distribution based Statistical Con.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\94U8VEWZ\\1903.html:text/html}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{franoischollet2017learning,
  added-at = {2018-08-01T08:16:18.000+0200},
  author = {Chollet, François},
  biburl = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  isbn = {9781617294433},
  keywords = {ai deeplearning ml},
  month = nov,
  publisher = {Manning},
  timestamp = {2018-08-01T08:16:18.000+0200},
  title = {Deep Learning with Python },
  year = 2017
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://arxiv.org/abs/1512.03385v1},
	abstract = {Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57\% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28\% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2020-10-23},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-10-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}


@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {https://arxiv.org/abs/1409.1556v6},
	abstract = {In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters, which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission, where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets, where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision.},
	language = {en},
	urldate = {2020-10-23},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\pbran\\Zotero\\storage\\L4AZTJFT\\Simonyan e Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\DTQDAHD5\\1409.html:text/html}
}

@article{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	language = {en},
	urldate = {2020-10-23},
	author = {Nielsen, Michael A.},
	year = {2015},
	note = {Publisher: Determination Press},
	file = {Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\35IR2NGM\\about.html:text/html}
}

@article{blalock_what_2020,
	title = {What is the {State} of {Neural} {Network} {Pruning}?},
	url = {http://arxiv.org/abs/2003.03033},
	abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	urldate = {2020-10-23},
	journal = {arXiv:2003.03033 [cs, stat]},
	author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03033},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in Proceedings of Machine Learning and Systems 2020 (MLSys 2020)},
	file = {arXiv Fulltext PDF:C\:\\Users\\pbran\\Zotero\\storage\\ZNN94ZLY\\Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pbran\\Zotero\\storage\\YKFYW452\\2003.html:text/html}
}

